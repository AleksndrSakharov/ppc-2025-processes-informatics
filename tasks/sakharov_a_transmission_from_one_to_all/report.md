# Передача от одного всем (broadcast)

- Student: Сахаров Александр Владимирович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Введение
Коллективные операции являются важной частью стандарта MPI, обеспечивая удобные и эффективные способы взаимодействия групп процессов. Одной из самых фундаментальных операций является `MPI_Bcast` — рассылка данных от одного процесса (корня) всем остальным процессам в коммуникаторе.

В данной работе реализована собственная версия операции Broadcast, использующая только базовые функции `MPI_Send` и `MPI_Recv`. Реализация выполнена на основе древовидной топологии передачи данных, что позволяет достичь логарифмической сложности алгоритма рассылки $O(\log P)$, где $P$ — число процессов.

## 2. Постановка задачи
**Формальная постановка:** Реализовать функцию передачи данных от одного процесса всем остальным (broadcast), используя только функции `MPI_Send` и `MPI_Recv`.

**Требования:**
- Реализованная функция должна иметь тот же прототип, что и стандартная функция `MPI_Bcast`.
- Тестовая программа должна позволять выбирать номер процесса `root`.
- Поддержка передачи массивов типов: `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`.
- Передача должна выполняться с использованием «дерева» процессов.

**Входные данные:**
- Массив данных на процессе `root`.
- Номер корневого процесса `root`.

**Выходные данные:**
- Тот же массив данных, доступный на всех процессах коммуникатора.

## 3. Последовательная версия
В контексте данной задачи "последовательная" версия (SEQ) представляет собой вырожденный случай, когда данные уже находятся в памяти единственного процесса. Поскольку пересылка данных внутри одного процесса не требуется (или сводится к копированию памяти), последовательная реализация выполняет простое присваивание входных данных выходным.

```cpp
GetOutput() = GetInput();
```

Она служит эталоном корректности данных (валидации), но не несет вычислительной нагрузки, характерной для распределенной задачи.

## 4. Параллельная версия

### 4.1. Алгоритм (Древовидная рассылка)
Для реализации эффективной рассылки используется схема бинарного дерева. Процессы логически выстраиваются в дерево, где корнем является процесс-отправитель (`root`).

**Логика построения дерева:**
Для упрощения расчетов рангов в дереве используется понятие "виртуального ранга" (`v_rank`), где корень всегда имеет `v_rank = 0`.
- `v_rank = (rank - root + size) % size`

Для узла с виртуальным рангом `v`:
- Левый потомок: `2 * v + 1`
- Правый потомок: `2 * v + 2`
- Родитель: `(v - 1) / 2`

**Алгоритм работы процесса:**
1. Вычислить свой `v_rank`.
2. Если `v_rank != 0` (не корень), ожидать получения данных от родителя (`MPI_Recv`).
3. Вычислить ранги потомков.
4. Если существуют потомки (их `v_rank < size`), отправить им полученные данные (`MPI_Send`).

Обратное преобразование виртуального ранга в реальный для отправки/получения:
- `real_rank = (v_rank + root) % size`

### 4.2. Реализация функции My_Bcast
Функция полностью повторяет сигнатуру `MPI_Bcast`:
```cpp
int My_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm) {
    // ... вычисление рангов ...
    
    // 1. Получение от родителя
    if (v_rank != 0) {
        MPI_Recv(buffer, count, datatype, real_parent, ...);
    }

    // 2. Отправка потомкам
    if (v_child1 < size) MPI_Send(buffer, count, datatype, real_child1, ...);
    if (v_child2 < size) MPI_Send(buffer, count, datatype, real_child2, ...);

    return MPI_SUCCESS;
}
```

### 4.3. Структура MPI задачи
Класс `SakharovATransmissionFromOneToAllMPI` инкапсулирует тестирование разработанной функции:
1. **Validation:** Проверка корректности номера `root`.
2. **Run:**
   - Процесс `root` подготавливает данные.
   - Вызывается `My_Bcast` для размера данных (чтобы все процессы знали, сколько памяти выделить).
   - Процессы выделяют память.
   - Вызывается `My_Bcast` для самих данных.
   - В рамках теста проверяется корректность передачи для типов `int`, `float`, `double`.

## 5. Детали реализации

### 5.1. Файловая структура
`sakharov_a_transmission_from_one_to_all/`:
- `common/include/common.hpp` — общие типы данных (`std::vector<int>` и др.).
- `mpi/include/ops_mpi.hpp` — объявление `My_Bcast` и класса задачи.
- `mpi/src/ops_mpi.cpp` — реализация алгоритма древовидной рассылки.
- `seq/src/ops_seq.cpp` — заглушка последовательной версии.
- `tests/` — функциональные и нагрузочные тесты.

## 6. Экспериментальное окружение

**Аппаратная платформа / ОС:**
- CPU: Intel Core i5-12400F (6 cores, 12 threads, 2.50 GHz);
- RAM: 32 GB DDR4;
- ОС: Windows 10 + WSL(Ubuntu 24.04.3 LTS).

**Инструменты:**
- Компилятор: `g++ 13.3.0`;
- MPI: `OpenMPI 4.1.6`;
- Сборка: `CMake 3.28.3`.

## 7. Результаты и обсуждение

### 7.1. Производительность

Тестирование проводилось на массиве из 10,000,000 элементов типа `int`.

| Mode | Time, s | Описание |
|------|---------|----------|
| **SEQ Pipeline** | 0.0168 | Локальное копирование (без коммуникаций) |
| **SEQ Task Run** | 0.0156 | Чистое время копирования |
| **MPI Pipeline** | 0.0771 | Полный цикл с инициализацией MPI задачи |
| **MPI Task Run** | 0.0754 | Время выполнения `My_Bcast` |

### 7.2. Анализ результатов
1. **Сравнение SEQ и MPI:**
   Последовательная версия выполняется значительно быстрее (0.0156с против 0.0754с). Это ожидаемый результат, так как SEQ версия выполняет операции только в локальной памяти одного процесса, не затрачивая время на сетевой стек, упаковку данных и синхронизацию процессов. MPI версия выполняет реальную пересылку данных между процессами (даже в пределах одного узла через shared memory), что вносит накладные расходы.

2. **Эффективность алгоритма:**
   Время 0.075с для рассылки ~40 МБ данных (10^7 int) на 6 процессах показывает работоспособность алгоритма. Использование древовидной структуры позволяет распараллелить процесс передачи: корень не отправляет данные каждому процессу последовательно ($O(P)$), а делегирует это потомкам ($O(\log P)$).

## 8. Заключение
В ходе работы была успешно реализована функция `My_Bcast`, выполняющая коллективную рассылку данных с использованием точечных обменов `MPI_Send` и `MPI_Recv`.
- Реализована поддержка произвольного корневого процесса.
- Алгоритм построен на базе бинарного дерева, что обеспечивает масштабируемость.
- Проверена корректность передачи различных типов данных (`int`, `float`, `double`).
- Тесты подтверждают корректность работы реализации.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 11.12.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию