# Умножение плотных матриц (Кэннон)

- Student: Сахаров Александр Владимирович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Введение
Задача умножения плотных квадратных матриц хорошо демонстрирует декомпозицию данных и коммуникации в MPI. Алгоритм Кэннона использует двумерную решётку процессов и циклические сдвиги блоков, что даёт сбалансированную нагрузку и $O(q)$ этапов обменов при решётке $q \times q$.

## 2. Постановка задачи
**Формальная постановка:** для матриц $A, B \in \mathbb{R}^{N \times N}$ вычислить $C = A \times B$ с помощью блочного алгоритма Кэннона.

**Входные данные:**
- размер $N$;
- два массива длиной $N^2$ (построчное хранение) с элементами `double` для $A$ и $B$.

**Выходные данные:**
- массив длиной $N^2$ с элементами матрицы $C$ в построчном виде.

**Ограничения и допущения:**
- $N > 0$, оба массива содержат ровно $N^2$ элементов;
- в MPI-версии используется максимальная квадратная подрешётка $q \times q$, где $q^2 \le P$, $N$ кратно $q$;
- процессы с рангами $\ge q^2$ не участвуют в вычислении, но получают результат через финальный broadcast.

## 3. Последовательная версия
SEQ-вариант — блочное умножение с кеш-эвристикой (~96 KiB на блок). Этапы:
- **Validation:** проверка формы входа ($N > 0$, длины массивов).
- **PreProcessing:** инициализация выходной матрицы нулями.
- **Run:** обход блоков $(ii, kk, jj)$, внутри — троекратные вложенные циклы; размер блока выбирается функцией `SelectBlockSize`.
- **PostProcessing:** не требуется.

## 4. Параллельная версия (MPI)

### 4.1. Раскладка данных и коммуникаторы
- Вычисляется $q = \lfloor \sqrt{P} \rfloor$, активных процессов $q^2$. Условие участия: $N \bmod q = 0$ и $rank < q^2$.
- Активные процессы формируют подкоммуникатор через `MPI_Comm_split`; остальные ранги получают результат готовым.
- В подкоммуникаторе создаётся декартова решётка $q \times q$ с периодами по обоим измерениям (`MPI_Cart_create`).
- Процесс 0 подкоммуникатора разрезает $A$ и $B$ на блоки $(N/q) \times (N/q)$ и раздаёт их `MPI_Scatter`.

### 4.2. Начальное выравнивание (Cannon alignment)
- Для координат $(row, col)$ в решётке блок $A$ сдвигается влево на `row` позиций, блок $B$ — вверх на `col` позиций через `MPI_Sendrecv_replace` с шагом > 1.

### 4.3. Основной цикл вычислений
Для $step = 0..q-1$ выполняется:
- локальное блоковое умножение `c_block += a_block * b_block`;
- циклический сдвиг $A$ влево на 1 и $B$ вверх на 1 (периодические границы).

### 4.4. Сбор и распространение результата
- `MPI_Gather` возвращает блоки $C$ в процесс 0 подкоммуникатора, где они размещаются в глобальной матрице.
- Один `MPI_Bcast` из процесса 0 `MPI_COMM_WORLD` рассылает итог всем рангам (удобно для тестов и неактивных процессов).

### 4.5. Псевдокод MPI-версии

```pseudocode
RunImpl():
	get n, rank, world_size
	if world_size == 1: return BlockMultiply

	q = floor(sqrt(world_size))
	active = q * q
	participate = (q > 0) and (n mod q == 0) and (rank < active)

	sub_comm = CommSplit(participate)
	if not participate:
		if rank == 0: output = BlockMultiply
		Bcast(output, root=0, comm=WORLD)
		return true

	cart_comm = CartCreate(sub_comm, dims=[q, q], periods=[1, 1])
	coords = CartCoords(cart_comm, sub_rank)
	block_size = n / q; block_elems = block_size^2

	Scatter blocks of A,B to a_block, b_block
	ShiftLeftN(a_block, coords.row); ShiftUpN(b_block, coords.col)

	for step in 0..q-1:
		LocalMatMul(a_block, b_block, c_block)
		ShiftLeft(a_block); ShiftUp(b_block)

	Gather c_block to root in sub_comm
	if sub_rank == 0: assemble blocks into output
	Bcast(output, root=0, comm=WORLD)
	return true
```

## 5. Детали реализации

### 5.1. Файловая структура проекта
`tasks/sakharov_a_cannon_algorithm/`:
- `common/include/common.hpp` — типы `MatrixTaskInput`, `InType`, `OutType`, функции `Offset`, `HasValidShape`, `SelectBlockSize`, блочное умножение `BlockMultiply`.
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательный таск с валидацией, подготовкой буфера и вызовом `BlockMultiply`.
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — реализация Каннона: формирование решётки, scatter блоков, начальные сдвиги, главный цикл, gather и финальный broadcast.
- `tests/functional/main.cpp` — функциональные тесты с наивным эталоном.
- `tests/performance/main.cpp` — производительные тесты, размер матрицы выбирается как $128 \times q$.

### 5.2. Ключевые классы и функции
- `SakharovACannonAlgorithmSEQ` — SEQ-задача, запускает блочное умножение.
- `SakharovACannonAlgorithmMPI` — MPI-версия Каннона с поддержкой неучаствующих рангов и финальным broadcast.
- `BlockMultiply`/`MultiplyTile` — блочная реализация умножения, используемая и в SEQ, и в MPI (для fallback).
- `ComputeGridDim`, `CreateSubCommunicator`, `CreateCartesian` — построение решётки процессов.
- `ShiftLeft(N)`, `ShiftUp(N)` — одношаговые и многократные циклические сдвиги через `MPI_Sendrecv_replace`.
- `BuildScatterBuffers`/`AssembleBlocks` — упаковка блоков для scatter и восстановление глобальной матрицы после gather.

### 5.3. Использование памяти
- **SEQ:** хранит $A$, $B$ и выход $C$ целиком; дополнительная память — временный блоковый буфер в пределах $O(N^2)$.
- **MPI:** каждый активный процесс хранит по одному блоку $A$, $B$, $C$ размером $(N/q)^2$; процесс 0 подкоммуникатора временно хранит буферы scatter/gather размером $q^2$ блоков; неучаствующие ранги держат только финальный результат.

## 6. Экспериментальное окружение

**Аппаратная платформа / ОС:**
- CPU: Intel Core i5-12400F (6 cores, 12 threads, 2.50 GHz);
- RAM: 32 GB DDR4;
- Накопитель: SSD 512 GB;
- ОС: Windows 10 + WSL (Ubuntu 24.04.3 LTS).

**Инструменты разработки:**
- Компилятор: `g++ 13.3.0`;
- MPI: `OpenMPI 4.1.6`;
- Сборка: `CMake 3.28.3`.

**Параметры запуска:**
- число процессов: 6 (`mpirun -n 6 ...`);
- размер матрицы в перф-тестах: $N = 128 \times q$, где $q = \lfloor \sqrt{P} \rfloor$.

## 7. Результаты и обсуждение

### 7.1. Корректность
- Функциональные тесты сравнивают SEQ и MPI с наивным эталоном для наборов $1\times1$, $2\times2$, $3\times3$ (единичная матрица), $4\times4$.
- В MPI учитывается сценарий $P$ не-квадрат: лишние ранги не участвуют в вычислении, но получают результат через broadcast.
- Проверка входа гарантирует соответствие размера и длины массивов.

### 7.2. Производительность

| Mode | Time, s | Speedup | Efficiency |
|------|---------|---------|------------|
| seq, pipeline_sakharov_a_cannon_algorithm_seq_enabled | 0.0051033497 | 1.00 | N/A |
| mpi, pipeline_sakharov_a_cannon_algorithm_mpi_enabled | 0.0027035274 | 1.89 | 31.4% |
| seq, task_run_sakharov_a_cannon_algorithm_seq_enabled | 0.0053810596 | 1.00 | N/A |
| mpi, task_run_sakharov_a_cannon_algorithm_mpi_enabled | 0.0033714170 | 1.59 | 26.6% |

**Интерпретация:**
- MPI выигрывает за счёт распараллеливания: ускорение 1.9x (pipeline) и 1.6x (task_run) на 6 процессах.
- Эффективность 26–31% отражает накладные расходы на scatter/gather и циклические сдвиги; рост $N$ и полнота квадратной решётки улучшат показатели.
- Режим `pipeline` немного быстрее `task_run` из-за разных стадий подготовки/сборки, но разница невелика.

## 8. Заключение
Реализованы SEQ и MPI-версии алгоритма Кэннона. MPI-вариант использует решётку процессов, стартовое выравнивание и циклические сдвиги блоков, обеспечивает корректность при неквадратном числе процессов и демонстрирует ускорение на выбранной конфигурации. Дальнейшие шаги: подстройка размера блока под архитектуру и сокращение финального broadcast при отсутствии необходимости в результате на всех рангах.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 20.11.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию