# Умножение плотных матриц (Кэннон)

- Student: Сахаров Александр Владимирович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Введение
Рассматривается умножение двух плотных квадратных матриц типа `double` методом Кэннона с блочной схемой. Алгоритм развернут для последовательной версии (блочное умножение в одном процессе) и параллельной версии MPI (двумерная декартова решётка процессов, циклические сдвиги блоков и локальные блоковые перемножения).

## 2. Постановка задачи
**Формальная постановка:**
- Даны квадратные матрицы $A, B \in \mathbb{R}^{N \times N}$.
- Требуется вычислить $C = A \times B$ блочным алгоритмом Кэннона.

**Условия и допущения:**
- Матрицы плотные, элементы типа `double`.
- В MPI-версии используется максимально возможная квадратная подрешётка $q \times q$, где $q^2 \le P$ (P — количество процессов), $N$ кратно $q$.
- Некратные ранги (если $P$ не является квадратом) не участвуют в вычислении, но получают результат через broadcast.

**Входные данные:**
- Размер $N$ и два массива длиной $N^2$ (построчное хранение) для $A$ и $B$.

**Выходные данные:**
- Массив длиной $N^2$ — матрица $C$ в построчном виде.

## 3. Последовательная версия (SEQ)
- Валидация: проверка корректности размера и наличия $N^2$ элементов в обеих матрицах.
- Предобработка: инициализация выходного массива нулями.
- Исполнение: блочное умножение с подбором размера блока из эвристики по кешу (цель ≈96 KiB). Вложенные циклы по блокам $(ii, kk, jj)$ и по внутренним индексам $(i, k, j)$.
- Постобработка: не требуется.

## 4. Параллельная версия (MPI)

### 4.1. Раскладка и коммуникации
- Формируется квадратная решётка процессов $q \times q$ (Cartesian topology, периодическая по обоим измерениям).
- Используется только подмножество процессов с рангами < $q^2$; остальные исключаются через `MPI_Comm_split` и получают готовый результат из процесса 0 после вычисления.
- Матрицы $A$ и $B$ разбиваются на $q^2$ блоков размера $(N/q) \times (N/q)$. Блоки рассылются за один проход `MPI_Scatter` для $A$ и $B` в подкоммуникаторе.

### 4.2. Начальное выравнивание (Cannon alignment)
- Одноразовые циклические сдвиги: блоки $A$ сдвигаются влево на `row` шагов, блоки $B$ — вверх на `col` шагов (`MPI_Sendrecv_replace` с шагом > 1 за один обмен).

### 4.3. Вычисление
- Для каждого из $q$ шагов выполняется локальное блоковое умножение и циклические сдвиги $A$ влево, $B$ вверх на 1 позицию.
- Внутри блока умножение реализовано как классическое троекратное вложение циклов по блоку.

### 4.4. Сбор результата
- `MPI_Gather` собирает все блоки $C$ в процессе 0 подкоммуникатора; там блоки размещаются в глобальной матрице.
- Один `MPI_Bcast` распространяет итоговую матрицу на весь `MPI_COMM_WORLD` (для удобства тестов).

## 5. Файловая структура
- `common/include/common.hpp` — типы входа/выхода, проверка корректности входа, выбор размера блока, локальное блочное умножение.
- `seq/include|src/ops_seq.*` — последовательная реализация (блочное умножение).
- `mpi/include|src/ops_mpi.*` — MPI-реализация Каннона: формирование подрешётки, раздача блоков, циклические сдвиги, сбор.
- `tests/functional/main.cpp` — функциональные тесты с наивной проверкой результата.
- `tests/performance/main.cpp` — нагрузочные тесты: размер матрицы = `grid_dim * 128`, референс — наивное умножение.

## 6. Экспериментальное окружение
- CPU: Intel Core i5-12400F (6C/12T, 2.50 GHz)
- RAM: 32 GB DDR4
- ОС: Windows 10 + WSL (Ubuntu 24.04.3 LTS)
- Компилятор: g++ 13.3.0; MPI: OpenMPI 4.1.6; Сборка: CMake 3.28.3

## 7. Результаты
### 7.1. Производительность

Тесты производительности (`SakharovARunPerfTestProcesses`) запускаются в двух режимах:
- `pipeline` — время полной обработки (Validation + PreProcessing + Run + PostProcessing);
- `task_run` — время многократного вызова только `Run()` с фиксированными входными данными.

Пример результатов (6 MPI-процессов, крупный размер матрицы):

| Mode | Time, s | Speedup | Efficiency |
|------|---------|---------|------------|
| seq, pipeline_sakharov_a_cannon_algorithm_seq_enabled | 0.0051033497 | 1.00 | N/A |
| mpi, pipeline_sakharov_a_cannon_algorithm_mpi_enabled | 0.0027035274 | 1.89 | 31.4% |
| seq, task_run_sakharov_a_cannon_algorithm_seq_enabled | 0.0053810596 | 1.00 | N/A |
| mpi, task_run_sakharov_a_cannon_algorithm_mpi_enabled | 0.0033714170 | 1.59 | 26.6% |

### 7.2. Анализ результатов
- MPI быстрее SEQ в обоих режимах; ускорение 1.9x (pipeline) и 1.6x (task_run) на 6 процессах.
- Эффективность 27–31% отражает накладные расходы на scatter/gather и финальный broadcast; рост размера матрицы и квадратной решётки улучшает метрики.

### 7.3. Узкие места
- Коммуникации: `MPI_Scatter`, циклические `Sendrecv`, `MPI_Gather` и финальный `MPI_Bcast` дают заметную долю времени.
- Память и кеш: блочная схема снижает промахи, но при одном узле процессы конкурируют за кеши/пропускную способность.
- Неиспользуемые ранги: если число процессов не квадрат, часть рангов простаивает, а результат всё равно рассылается всему `MPI_COMM_WORLD` (дополнительный broadcast).

## 8. Заключение
- Реализован блочный алгоритм Кэннона для плотных матриц `double` в вариантах SEQ и MPI.
- Оптимизации: крупные кеш-блоки (~96 KiB), групповые scatter/gather, одношаговые стартовые сдвиги, выбор квадратной подрешётки процессов и единый broadcast результата.
- Для дальнейшего ускорения можно убирать финальный broadcast, если результат нужен только процессу 0, и подбирать размер блоков под конкретную архитектуру.

## 9. Источники
1. Cannon L.E. A Cellular Computer to Implement the Kalman Filter // PhD Thesis, Montana State University, 1969.
2. Gropp W., Lusk E., Skjellum A. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, 2014.
3. Open MPI Documentation – https://www.open-mpi.org/doc/